FAILED tests/unit/data/test_venue_collection_engine.py::TestVenueCollectionEngine::test_api_failure_recovery - AssertionError: assert 0 > 0
 +  where 0 = len([])
 +    where [] = BatchCollectionResult(papers=[], venues_attempted=['ICML', 'NeurIPS'], venues_successful=[], venues_failed=['ICML', 'NeurIPS'], year=2023, collection_metadata={}, total_duration_seconds=1.845652, errors=[APIError(error_type='client_error', message='OpenAlex client error: 403', status_code=403, timestamp=datetime.datetime(2025, 7, 8, 8, 14, 38, 743690))]).venues_successful
FAILED tests/unit/data/test_venue_collection_engine.py::TestVenueCollectionEngineIntegration::test_four_to_six_hour_collection_scenario - assert 27 <= (150 * 0.15)
 +  where 27 = CollectionEstimate(total_batches=27, estimated_duration_hours=0.0075, expected_paper_count=15000, api_calls_required=27).api_calls_required
FAILED tests/unit/data/test_venue_normalizer.py::TestFuzzyVenueMatcher::test_normalize_venue_name - AssertionError: assert 'PROCEEDINGS ICLR' == 'ICLR'

  - ICLR
  + PROCEEDINGS ICLR
FAILED tests/unit/data/test_venue_normalizer.py::TestFuzzyVenueMatcher::test_find_best_match - AssertionError: assert 'exact' == 'fuzzy'

  - fuzzy
  + exact
FAILED tests/unit/data/test_venue_normalizer.py::TestFuzzyVenueMatcher::test_batch_find_matches - AssertionError: assert None == 'ICML'
 +  where None = FuzzyMatchResult(original_venue='ICML 2024', matched_venue=None, similarity_score=0.0, normalized_original='ICML', normalized_matched=None, match_type='none').matched_venue
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_initialization - assert None is not None
 +  where None = <compute_forecast.testing.error_injection.recovery_validator.RecoveryValidator object at 0x79598e0230e0>.recovery_engine
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_validate_recovery_successful - AssertionError: assert 12.5 == 0.0
 +  where 12.5 = RecoveryMetrics(error_type=<ErrorType.API_TIMEOUT: 'api_timeout'>, recovery_attempted=True, recovery_successful=True, recovery_time_seconds=300.00001, data_loss_percentage=12.5, partial_results_available=True, component='api_client', error_details=None).data_loss_percentage
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_validate_recovery_with_data_loss - AssertionError: assert 7.2222222222222285 == 5.0
 +  where 7.2222222222222285 = RecoveryMetrics(error_type=<ErrorType.DATA_CORRUPTION: 'data_corruption'>, recovery_attempted=True, recovery_successful=True, recovery_time_seconds=0.0, data_loss_percentage=7.2222222222222285, partial_results_available=True, component='parser', error_details=None).data_loss_percentage
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_validate_recovery_failed - AssertionError: assert 50.0 == 100.0
 +  where 50.0 = RecoveryMetrics(error_type=<ErrorType.COMPONENT_CRASH: 'component_crash'>, recovery_attempted=True, recovery_successful=False, recovery_time_seconds=0.0, data_loss_percentage=50.0, partial_results_available=False, component='analyzer', error_details=None).data_loss_percentage
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_measure_data_integrity - assert 85.41666666666666 == 75.0
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_verify_graceful_degradation_healthy - AttributeError: 'NoneType' object has no attribute 'get_recovery_status' and no __dict__ for setting new attributes
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_verify_graceful_degradation_degraded - AttributeError: 'NoneType' object has no attribute 'get_recovery_status' and no __dict__ for setting new attributes
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_verify_graceful_degradation_failed - AttributeError: 'NoneType' object has no attribute 'get_recovery_status' and no __dict__ for setting new attributes
FAILED tests/unit/error_injection/test_recovery_validator.py::TestRecoveryValidator::test_recommendations_generation - assert False
 +  where False = any(<generator object TestRecoveryValidator.test_recommendations_generation.<locals>.<genexpr> at 0x79597d1d5630>)
FAILED tests/unit/extraction/test_extraction_protocol.py::TestExtractionProtocol::test_phase1_preparation - AssertionError: assert 0 > 0
 +  where 0 = ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 580318), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.PREPARATION: 'preparation'>).time_spent_minutes
 +    where ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 580318), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.PREPARATION: 'preparation'>) = ExtractionResult(metadata=ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 580318), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.PREPARATION: 'preparation'>), automated_extraction={}, hardware=HardwareSpecs(gpu_type=None, gpu_count=None, gpu_memory_gb=None, tpu_version=None, tpu_cores=None, nodes_used=None, special_hardware=None), training=TrainingSpecs(total_time_hours=None, time_unit_original=None, pre_training_hours=None, fine_tuning_hours=None, number_of_runs=1, wall_clock_time=None), model=ModelSpecs(parameters_count=None, parameters_unit='millions', architecture=None, layers=None, hidden_size=None, model_size_gb=None), dataset=DatasetSpecs(name=None, size_gb=None, samples_count=None, tokens_count=None, batch_size=None, sequence_length=None), computation=ComputationSpecs(total_gpu_hours=None, calculation_method=None, estimated_cost_usd=None, flops_estimate=None), validation=ValidationResults(confidence_hardware=<ConfidenceLevel.LOW: 'low'>, confidence_training=<ConfidenceLevel.LOW: 'low'>, confidence_model=<ConfidenceLevel.LOW: 'low'>, confidence_overall=<ConfidenceLevel.LOW: 'low'>, consistency_checks_passed=False, outliers_flagged=[]), notes=ExtractionNotes(ambiguities=[], assumptions=[], follow_up_needed=[], quality_issues=[])).metadata
 +      where ExtractionResult(metadata=ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 580318), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.PREPARATION: 'preparation'>), automated_extraction={}, hardware=HardwareSpecs(gpu_type=None, gpu_count=None, gpu_memory_gb=None, tpu_version=None, tpu_cores=None, nodes_used=None, special_hardware=None), training=TrainingSpecs(total_time_hours=None, time_unit_original=None, pre_training_hours=None, fine_tuning_hours=None, number_of_runs=1, wall_clock_time=None), model=ModelSpecs(parameters_count=None, parameters_unit='millions', architecture=None, layers=None, hidden_size=None, model_size_gb=None), dataset=DatasetSpecs(name=None, size_gb=None, samples_count=None, tokens_count=None, batch_size=None, sequence_length=None), computation=ComputationSpecs(total_gpu_hours=None, calculation_method=None, estimated_cost_usd=None, flops_estimate=None), validation=ValidationResults(confidence_hardware=<ConfidenceLevel.LOW: 'low'>, confidence_training=<ConfidenceLevel.LOW: 'low'>, confidence_model=<ConfidenceLevel.LOW: 'low'>, confidence_overall=<ConfidenceLevel.LOW: 'low'>, consistency_checks_passed=False, outliers_flagged=[]), notes=ExtractionNotes(ambiguities=[], assumptions=[], follow_up_needed=[], quality_issues=[])) = <compute_forecast.analysis.computational.extraction_protocol.ExtractionProtocol object at 0x79598d74efd0>.extraction_result
FAILED tests/unit/extraction/test_extraction_protocol.py::TestExtractionProtocol::test_run_full_protocol - AssertionError: assert 0 > 0
 +  where 0 = ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 597923), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.DOCUMENTATION: 'documentation'>).time_spent_minutes
 +    where ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 597923), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.DOCUMENTATION: 'documentation'>) = ExtractionResult(metadata=ExtractionMetadata(paper_id='test_paper_001', title='', extraction_date=datetime.datetime(2025, 7, 8, 8, 14, 41, 597923), analyst='test_analyst', time_spent_minutes=0, phase_completed=<ExtractionPhase.DOCUMENTATION: 'documentation'>), automated_extraction={'confidence_score': 0.75, 'fields_found': ['_mock_return_value', '_mock_parent', '_mock_name', '_mock_new_name', '_mock_new_parent', '_mock_sealed', '_spec_class', '_spec_set', '_spec_signature', '_mock_methods', '_spec_asyncs', '_mock_children', '_mock_wraps', '_mock_delegate', '_mock_called', '_mock_call_args', '_mock_call_count', '_mock_call_args_list', '_mock_mock_calls', 'method_calls', '_mock_unsafe', 'confidence', 'gpu_type', 'gpu_count', 'training_hours', '_mock_side_effect'], 'fields_missing': [], 'raw_results': <Mock name='mock.analyze()' id='133425256963040'>}, hardware=HardwareSpecs(gpu_type='V100', gpu_count=None, gpu_memory_gb=None, tpu_version=None, tpu_cores=None, nodes_used=None, special_hardware=None), training=TrainingSpecs(total_time_hours=None, time_unit_original='hours', pre_training_hours=None, fine_tuning_hours=None, number_of_runs=1, wall_clock_time=None), model=ModelSpecs(parameters_count=None, parameters_unit='millions', architecture='Transformer', layers=None, hidden_size=None, model_size_gb=None), dataset=DatasetSpecs(name='IMAGENET', size_gb=None, samples_count=None, tokens_count=None, batch_size=None, sequence_length=None), computation=ComputationSpecs(total_gpu_hours=None, calculation_method='explicit_gpu_hours', estimated_cost_usd=None, flops_estimate=None), validation=ValidationResults(confidence_hardware=<ConfidenceLevel.LOW: 'low'>, confidence_training=<ConfidenceLevel.LOW: 'low'>, confidence_model=<ConfidenceLevel.LOW: 'low'>, confidence_overall=<ConfidenceLevel.LOW: 'low'>, consistency_checks_passed=True, outliers_flagged=[]), notes=ExtractionNotes(ambiguities=[], assumptions=[], follow_up_needed=[], quality_issues=[])).metadata
FAILED tests/unit/extraction/test_extraction_protocol.py::TestExtractionProtocol::test_calculate_completeness_score - assert 0.06896551724137931 == 0.0
FAILED tests/unit/extraction/test_extraction_protocol.py::TestExtractionProtocolEdgeCases::test_missing_analyzer_attributes - AssertionError: assert <Mock name='mock.analyze().confidence' id='133425256962368'> == 0.0
FAILED tests/unit/orchestration/test_enhanced_orchestrator.py::TestEnhancedCollectionOrchestrator::test_initialization_default - AssertionError: expected call not found.
Expected: GoogleScholarClient(use_proxy=False)
  Actual: GoogleScholarClient()
FAILED tests/unit/orchestration/test_enhanced_orchestrator.py::TestEnhancedCollectionOrchestrator::test_initialization_with_api_keys - AssertionError: expected call not found.
Expected: GoogleScholarClient(use_proxy=True)
  Actual: GoogleScholarClient()
FAILED tests/unit/orchestration/test_enhanced_orchestrator_env.py::TestEnhancedOrchestratorEnvironment::test_env_vars_override_by_init_params - AttributeError: <module 'compute_forecast.data.sources.google_scholar' from '/home/bouthilx/projects/compute_forecast_sandbox/preliminary_report/compute_forecast/data/sources/google_scholar.py'> does not have the attribute 'GoogleScholarClient'
FAILED tests/unit/orchestration/test_enhanced_orchestrator_env.py::TestEnhancedOrchestratorEnvironment::test_api_client_initialization_with_env_vars - AttributeError: <module 'compute_forecast.data.sources.google_scholar' from '/home/bouthilx/projects/compute_forecast_sandbox/preliminary_report/compute_forecast/data/sources/google_scholar.py'> does not have the attribute 'GoogleScholarClient'
FAILED tests/unit/orchestration/test_rate_limit_manager.py::TestRateLimitManagerIntegration::test_exponential_backoff_for_degraded_apis - AssertionError: Should have some non-zero wait times
assert 0 > 0
 +  where 0 = len([])
FAILED tests/unit/orchestration/test_rate_limit_manager.py::TestRateLimitManagerIntegration::test_realistic_collection_scenario - assert 100 > 100
FAILED tests/unit/orchestration/test_state_management.py::TestStateManager::test_state_manager_initialization - assert None is not None
 +  where None = <compute_forecast.data.collectors.state_management.StateManager object at 0x79598d60e670>.persistence
FAILED tests/unit/orchestration/test_state_management.py::TestStateManager::test_create_session_basic - AssertionError: assert False
 +  where False = exists()
 +    where exists = (PosixPath('/tmp/tmpdoxee50e/sessions/session_48608cef') / 'session.json').exists
FAILED tests/unit/orchestration/test_state_management.py::TestStateManager::test_resume_session - AssertionError: assert False is True
 +  where False = SessionResumeResult(session_id='session_f4ecd15c', plan_id='recovery_session_f4ecd15c_1751976933', success=False, recovery_start_time=datetime.datetime(2025, 7, 8, 8, 15, 33, 521760), recovery_end_time=datetime.datetime(2025, 7, 8, 8, 15, 33, 521760), recovery_duration_seconds=0.0, checkpoints_recovered=0, papers_recovered=0, venues_recovered=0, data_files_recovered=0, recovery_steps_executed=[], recovery_steps_failed=[], data_integrity_checks=[], state_consistency_validated=False, checkpoint_validation_results=[], session_state_after_recovery=None, ready_for_continuation=False, resume_warnings=[], resume_errors=['Session session_f4ecd15c not found'], partial_recovery_issues=[]).success
FAILED tests/unit/orchestration/test_state_management.py::TestStateManager::test_get_session_status - assert None is not None
FAILED tests/unit/orchestration/test_state_management.py::TestStateManager::test_cleanup_old_sessions - AttributeError: 'NoneType' object has no attribute 'save_state_atomic'
FAILED tests/unit/pdf_discovery/test_framework.py::TestPDFDiscoveryFramework::test_discover_pdfs_with_failures - AssertionError: assert 'openreview' == 'arxiv'

  - arxiv
  + openreview
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_file_exists - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_file_not_exists - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_initialization - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_test_connection_failure - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_test_connection_success - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_upload_file_failure - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_google_drive_store.py::TestGoogleDriveStore::test_upload_file_success - AttributeError: module 'compute_forecast.pdf_storage.google_drive_store' has no attribute 'service_account'
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_cleanup_cache - TypeError: PDFManager.__init__() got an unexpected keyword argument 'cache_ttl_hours'. Did you mean 'cache_ttl_days'?
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_get_pdf_for_analysis_cached - AttributeError: module 'compute_forecast.pdf_storage.pdf_manager' has no attribute 'requests'
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_get_pdf_for_analysis_download - AttributeError: module 'compute_forecast.pdf_storage.pdf_manager' has no attribute 'requests'
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_get_statistics - AssertionError: 0 != 1
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_store_pdf_failure - AssertionError: True is not false
FAILED tests/unit/pdf_storage/test_pdf_manager.py::TestPDFManager::test_store_pdf_success - AssertionError: Expected 'upload_file' to have been called once. Called 0 times.
FAILED tests/unit/quality/extraction/test_consistency_checker.py::TestExtractionConsistencyChecker::test_cross_paper_consistency_high_variation - AssertionError: assert True is False
 +  where True = ConsistencyCheck(check_type='cross_paper', passed=True, confidence=np.float64(0.7477554666025645), details={'coefficient_of_variation': np.float64(1.5224453339743544), 'outliers': 0, 'total': 10, 'consistent': True}).passed
FAILED tests/unit/quality/extraction/test_consistency_checker.py::TestExtractionConsistencyChecker::test_scaling_consistency_violation - AssertionError: assert True is False
 +  where True = ConsistencyCheck(check_type='scaling_law', passed=True, confidence=0.9, details={'follows_scaling_law': True, 'ratio': 5.011872336272727}).passed
FAILED tests/unit/quality/extraction/test_consistency_checker.py::TestExtractionConsistencyChecker::test_determine_domain - AssertionError: assert 'nlp' == 'cv'

  - cv
  + nlp
FAILED tests/unit/quality/extraction/test_integrated_validator.py::TestIntegratedExtractionValidator::test_validate_extraction_batch - numpy.linalg.LinAlgError: SVD did not converge in Linear Least Squares
FAILED tests/unit/quality/extraction/test_integrated_validator.py::TestIntegratedExtractionValidator::test_outlier_detection_integration - assert 0 > 0
 +  where 0 = len([])
 +    where [] = IntegratedValidationResult(paper_id='test_paper_1', extraction_validation=ExtractionValidation(paper_id='test_paper_1', extraction_type='computational_analysis', extracted_value=MockComputationalAnalysis(gpu_hours=1000000, gpu_type=None, gpu_count=None, training_time=125000, parameters=7000000000.0, gpu_memory=None, batch_size=None, dataset_size=None, epochs=None, learning_rate=None, optimizer=None, framework=None, cost_estimate=None, model_size_gb=None), confidence=0.5895716945996277, quality=<ExtractionQuality.LOW: 'low'>, validation_method='weighted_scoring', cross_validation_result={'completeness': 0.30726256983240224, 'validity': 0.6666666666666666, 'consistency': 1.0}), consistency_checks=[ConsistencyCheck(check_type='domain_specific', passed=True, confidence=1.0, details={'domain': 'nlp', 'violations': [], 'checks_performed': 2, 'violation_rate': 0.0}), ConsistencyCheck(check_type='scaling_law', passed=False, confidence=0.4, details={'issue': 'scaling_law_violation', 'expected_gpu_hours': 7.790559126704483, 'actual_gpu_hours': 1000000, 'ratio': 128360.49168437724, 'log_ratio': np.float64(5.1084313719900205)}), ConsistencyCheck(check_type='cross_paper', passed=True, confidence=np.float64(0.8858578643762691), details={'coefficient_of_variation': np.float64(0.1414213562373095), 'outliers': 0, 'total': 25, 'consistent': True}), ConsistencyCheck(check_type='cross_paper', passed=True, confidence=np.float64(0.8898984745544779), details={'coefficient_of_variation': np.float64(0.10101525445522107), 'outliers': 0, 'total': 25, 'consistent': True}), ConsistencyCheck(check_type='cross_paper', passed=True, confidence=np.float64(0.8882148869802242), details={'coefficient_of_variation': np.float64(0.11785113019775792), 'outliers': 0, 'total': 25, 'consistent': True})], outlier_fields=[], overall_quality='low', recommendations=['Improve extraction completeness - missing critical fields', "GPU hours and parameters don't follow expected scaling", 'Verify extraction accuracy with additional sources'], confidence=np.float64(0.6325384597743976)).outlier_fields
FAILED tests/unit/quality/extraction/test_integrated_validator.py::TestIntegratedExtractionValidator::test_batch_cache_usage - numpy.linalg.LinAlgError: SVD did not converge in Linear Least Squares
FAILED tests/unit/quality/extraction/test_outlier_detection.py::TestOutlierDetector::test_contextualize_outlier_known_extreme - AssertionError: assert 'possible' == 'expected'

  - expected
  + possible
FAILED tests/unit/quality/extraction/test_outlier_detection.py::TestOutlierDetector::test_verify_outlier_corroborating_evidence - assert True is False
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_suppression_rule_creation - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_suppression_rule_expiry - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_matches_alert_by_title - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_matches_alert_by_message - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_matches_alert_by_rule_id - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_suppression.py::TestSuppressionRule::test_no_match_when_expired - TypeError: SuppressionRule.__init__() got an unexpected keyword argument 'pattern'
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_api_health_degraded_rule - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_collection_rate_low_rule - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_high_error_rate_rule - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_memory_usage_high_rule - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_metric_context_extraction - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestAlertRuleEvaluator::test_safe_evaluation_prevents_dangerous_code - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_alert_acknowledgment - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_alert_evaluation_performance - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_alert_resolution - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_alert_summary_generation - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_built_in_rules_loaded - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestIntelligentAlertSystem::test_performance_statistics - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestAlertSystemIntegration::test_alert_suppression_integration - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_alert_system.py::TestAlertSystemIntegration::test_end_to_end_alert_flow - TypeError: CollectionProgressMetrics.__init__() missing 11 required positional arguments: 'session_id', 'total_venues', 'completed_venues', 'in_progress_venues', 'failed_venues', 'papers_collected', 'estimated_total_papers', 'completion_percentage', 'session_duration_minutes', 'estimated_remaining_minutes', and 'estimated_completion_time'
FAILED tests/unit/quality/test_dashboard_metrics.py::TestDashboardMetrics::test_api_metrics - TypeError: APIMetrics.__init__() got an unexpected keyword argument 'total_requests'
FAILED tests/unit/quality/test_dashboard_metrics.py::TestDashboardMetrics::test_collection_progress_metrics - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestDashboardMetrics::test_system_metrics_to_dict - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestDashboardMetrics::test_venue_progress_metrics - TypeError: VenueProgressMetrics.__init__() missing 4 required positional arguments: 'completion_percentage', 'last_update_time', 'collection_duration_minutes', and 'estimated_remaining_minutes'
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_add_metrics - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_buffer_max_size - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_clear_buffer - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_get_metrics_history - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_get_metrics_since - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_dashboard_metrics.py::TestMetricsBuffer::test_thread_safety - TypeError: CollectionProgressMetrics.__init__() got an unexpected keyword argument 'total_papers_collected'. Did you mean 'papers_collected'?
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestNotificationChannels::test_channel_factory - AttributeError: 'dict' object has no attribute 'lower'
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestNotificationChannels::test_console_notification_channel - AttributeError: 'ConsoleNotificationChannel' object has no attribute 'test_connection'
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestNotificationChannels::test_dashboard_notification_channel - AttributeError: 'DashboardNotificationChannel' object has no attribute 'get_recent_alerts'
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntelligentAlertingSystem::test_initialization - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntelligentAlertingSystem::test_start_stop - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntelligentAlertingSystem::test_system_status - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntelligentAlertingSystem::test_test_alert_sending - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestAlertEscalation::test_escalation_rule_creation - ModuleNotFoundError: No module named 'monitoring'
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntegrationScenarios::test_api_health_alert_scenario - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_intelligent_alerting_system.py::TestIntegrationScenarios::test_system_resource_alert_scenario - TypeError: ConsoleNotificationChannel.__init__() takes from 1 to 2 positional arguments but 3 were given
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_initialization - AttributeError: 'MetricsCollector' object has no attribute 'is_collecting'
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_start_collection - TypeError: MetricsCollector.start_collection() missing 1 required positional argument: 'data_processors'
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_stop_collection - TypeError: MetricsCollector.start_collection() missing 1 required positional argument: 'data_processors'
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_collect_current_metrics_with_mocks - AttributeError: 'MetricsCollector' object has no attribute 'collect_current_metrics'. Did you mean: 'get_current_metrics'?
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_collect_current_metrics_performance - AttributeError: 'MetricsCollector' object has no attribute 'collect_current_metrics'. Did you mean: 'get_current_metrics'?
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_collect_current_metrics_error_handling - AttributeError: 'MetricsCollector' object has no attribute 'collect_current_metrics'. Did you mean: 'get_current_metrics'?
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_get_metrics_summary - AttributeError: 'MetricsCollector' object has no attribute 'get_metrics_summary'
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_collection_statistics - AttributeError: 'MetricsCollector' object has no attribute 'collection_stats'
FAILED tests/unit/quality/test_metrics_collector.py::TestMetricsCollector::test_collection_loop_integration - TypeError: MetricsCollector.start_collection() missing 1 required positional argument: 'data_processors'
FAILED tests/unit/test_package_configuration.py::TestPackageConfiguration::test_python_version_requirement - AssertionError: Expected Python 3.12 only, got >=3.12
assert '>=3.12' == '==3.12.*'

  - ==3.12.*
  + >=3.12
FAILED tests/unit/test_package_configuration.py::TestPackageConfiguration::test_python_classifiers - AssertionError: Expected only Python 3.12 classifiers, got ['Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.12', 'Programming Language :: Python :: 3.13']
assert {'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.13', 'Programming Language :: Python :: 3.12'} == {'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.12'}

  Extra items in the left set:
  'Programming Language :: Python :: 3.13'

  Full diff:
    {
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.12',
  +     'Programming Language :: Python :: 3.13',
    }
FAILED tests/unit/test_package_configuration.py::TestPackageConfiguration::test_current_python_version - AssertionError: Tests should run on Python 3.12, but running on 3.13
assert 13 == 12
 +  where 13 = sys.version_info(major=3, minor=13, micro=2, releaselevel='final', serial=0).minor
 +    where sys.version_info(major=3, minor=13, micro=2, releaselevel='final', serial=0) = sys.version_info
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_detect_interruption_type_network_failure - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_detect_interruption_type_api_timeout - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_detect_interruption_type_resource_exhaustion - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_detect_interruption_type_manual_stop - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_detect_interruption_type_system_crash - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_create_recovery_plan_network_failure - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_create_recovery_plan_api_timeout - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_create_recovery_plan_system_crash - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_create_recovery_plan_no_checkpoint - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_execute_recovery_resume_from_checkpoint - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_execute_recovery_retry_failed_venues - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_execute_recovery_partial - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_execute_recovery_failure - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_recovery_metrics_tracking - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_recovery_state_tracking - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_validate_recovery_capability_success - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_validate_recovery_capability_no_checkpoints - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_validate_recovery_capability_no_state - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_validate_recovery_capability_max_attempts - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_recovery_plan_generation_steps - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_checkpoint_age_validation - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
ERROR tests/unit/orchestration/test_interruption_recovery_system.py::TestInterruptionRecoverySystem::test_confidence_score_calculation - AttributeError: type object 'SessionState' has no attribute 'RUNNING'
