computational_indicators:
  gpu_hardware:
    - "GPU"
    - "V100"
    - "A100"
    - "H100"
    - "RTX"
    - "Tesla"
    - "TPU"
    - "CUDA"
    - "graphics processing unit"
    - "parallel processing"
    - "NVIDIA"
    - "tensor processing unit"

  training_resources:
    - "training time"
    - "compute hours"
    - "GPU hours"
    - "CPU hours"
    - "wall-clock time"
    - "training duration"
    - "computational cost"
    - "compute budget"
    - "resource consumption"
    - "energy consumption"

  model_scale:
    - "parameters"
    - "billion parameters"
    - "million parameters"
    - "model size"
    - "large model"
    - "transformer"
    - "neural network"
    - "deep network"
    - "architecture"
    - "layers"

  dataset_scale:
    - "dataset size"
    - "training data"
    - "million samples"
    - "billion tokens"
    - "large dataset"
    - "data preprocessing"
    - "batch size"
    - "mini-batch"
    - "data loading"
    - "storage requirements"

  optimization_compute:
    - "hyperparameter tuning"
    - "grid search"
    - "random search"
    - "neural architecture search"
    - "AutoML"
    - "optimization"
    - "cross-validation"
    - "ablation study"
    - "experimental validation"

  infrastructure:
    - "cluster"
    - "distributed"
    - "parallel"
    - "multi-gpu"
    - "multi-node"
    - "high performance computing"
    - "HPC"
    - "supercomputer"
    - "cloud platform"
    - "AWS"
    - "Google Cloud"
    - "Azure"
